# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/resnet.ipynb.

# %% auto 0
__all__ = []

# %% ../notebooks/resnet.ipynb 1
import torch
from torch import nn
from typing import Optional, List

# %% ../notebooks/resnet.ipynb 6
class Layer:
    def __init__(
        self,
        repeats: int,
        # inner loop
        out_chans: List[int],
        kernel_sizes: List[int],
        strides: List[int],
        paddings: List[int],
    ):
        self.repeats = repeats

        # inner loop
        self.out_chans = out_chans
        self.kernel_sizes = kernel_sizes
        self.strides = strides
        self.paddings = paddings


class ResNet(nn.Module):
    def __init__(self, layers: List[Layer]):
        super().__init__()

        # Initial
        self.conv_i = nn.Conv2d(64, 64, kernel_size=7, stride=2, padding=3)
        self.bn_conv_i = nn.BatchNorm2d(64)
        self.max_pool_i = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.bn_max_i = nn.BatchNorm2d(64)

        resnet_layers = []

        curr_chan = 64

        # Resnet
        for layer_idx, layer in enumerate(layers):
            strides = (
                [1, 1, 1] if layer_idx == 0 else [1, 2, 1]
            )  # TODO: remove hardcoding like this

            for i in range(layer.repeats):
                identity_chans = None

                resnet_block = []
                # inner loop
                for j, _ in enumerate(layer.out_chans):
                    if j == 0:
                        in_chan = curr_chan  # input is the output of previous layer ï¼ˆfrom prev block)
                        identity_chans = curr_chan
                    else:
                        in_chan = layer.out_chans[
                            j - 1
                        ]  # input is the output of previous layer

                    resnet_block.append(
                        nn.Conv2d(
                            in_chan,  # [64,64,64], [256,64,64] | [256,128,128], [512,128,128]
                            layer.out_chans[j],  # [64,64,256] | [128,128,512]
                            layer.kernel_sizes[j],  # [1,3,1]
                            strides[j],  # [1,1,1] | [1,2,1]
                            layer.paddings[j],  # [0,1,0]
                        )
                    )
                    resnet_block.append(nn.BatchNorm2d(layer.out_chans[j]))
                    curr_chan = layer.out_chans[j]

                #   output         residuals
                if curr_chan != identity_chans:
                    identity_downsample = [
                        nn.Conv2d(
                            in_channels=identity_chans,
                            out_channels=curr_chan,
                            kernel_size=1,
                            stride=max(
                                strides
                            ),  # if stride 2, then output is halved, so need to halve identity too
                        ),
                        nn.BatchNorm2d(curr_chan),
                    ]
                    resnet_block.append(nn.Sequential(*identity_downsample))

                resnet_block.append(nn.ReLU())
                resnet_layers.append(resnet_block)

        self.resnet_layers = resnet_layers

        # Ending
        self.avg_pool_e = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * 4, 1000)

    def forward(self, X):
        X = self.conv_i(X)
        X = self.bn_conv_i(X)
        X = self.max_pool_i(X)
        X = self.bn_max_i(X)

        identity = None
        for block in self.resnet_layers:
            for i, net in enumerate(block):
                if i == 0:
                    identity = X
                
                if isinstance(net, list):
                    
                X = net(X)

        X = self.avg_pool_e(X)
        X = self.fc(X)

# %% ../notebooks/resnet.ipynb 7
ResNet(
    [
        Layer(3, [64, 64, 256], [1, 3, 1], [1, 2, 1], [0, 1, 0]),
        Layer(4, [128, 128, 512], [1, 3, 1], [1, 2, 1], [0, 1, 0]),
        Layer(6, [256, 256, 1024], [1, 3, 1], [1, 2, 1], [0, 1, 0]),
        Layer(3, [512, 512, 2048], [1, 3, 1], [1, 2, 1], [0, 1, 0]),
    ]
)
