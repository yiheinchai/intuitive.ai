# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/resnet.ipynb.

# %% auto 0
__all__ = ['Layer', 'ResNet']

# %% ../notebooks/resnet.ipynb 1
import torch
from torch import nn
from typing import Optional, List

# %% ../notebooks/resnet.ipynb 5
class Layer:
    def __init__(
        self,
        repeats: int,

        # inner loop
        out_chans: List[int],
        kernel_sizes: List[int],
        strides: List[int],
        paddings: List[int],
    ):
        self.repeats = repeats

        # inner loop
        self.out_chans = out_chans
        self.kernel_sizes = kernel_sizes
        self.strides = strides
        self.paddings = paddings


class ResNet(nn.Module):
    def __init__(self, layers: List[Layer]):
        super().__init__()

        # Initial
        self.conv_i = nn.Conv2d(64, 64, kernel_size=7, stride=2, padding=3)
        self.bn_conv_i = nn.BatchNorm2d(64)
        self.max_pool_i = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.bn_max_i = nn.BatchNorm2d(64)

        resnet_layers = []

        curr_chan = 64
        final_in_chans = []

        # Resnet
        for layer_idx, layer in enumerate(layers):
            strides = [1,1,1] if layer_idx == 0 else [1,2,1]  # TODO: remove hardcoding like this
            final_in_chans_layer = [] # NOTE: remove later

            for i in range(layer.repeats):
                final_in_chans_repeat = []  # NOTE: remove later

                # inner loop
                for j, _ in enumerate(layer.out_chans):
                    in_chan = curr_chan if j == 0  else layer.out_chans[j - 1] # input is the output of previous layer
                    final_in_chans_repeat.append(in_chan)  # NOTE: remove later

                    resnet_layers.append(
                        nn.Conv2d(
                            in_chan, # [64,64,64], [256,64,64] | [256,128,128], [512,128,128]
                            layer.out_chans[j], # [64,64,256] | [128,128,512]
                            layer.kernel_sizes[j], # [1,3,1]
                            strides[j], # [1,1,1] | [1,2,1]
                            layer.paddings[j] # [0,1,0]
                        )
                    )
                    resnet_layers.append(nn.BatchNorm2d(layer.out_chans[j]))
                    curr_chan = layer.out_chans[j]

                #   output            residuals
                if curr_chan != final_in_chans_repeat[0]:
                    print("ME", curr_chan, final_in_chans_repeat[0])
                    resnet_layers.append(nn.Conv2d())
                resnet_layers.append(nn.ReLU())
                final_in_chans_layer.append(final_in_chans_repeat)  # NOTE: remove later
            final_in_chans.append(final_in_chans_layer)  # NOTE: remove later
        
        print(final_in_chans) 

        self.resnet_layers = resnet_layers

        # Ending
        self.avg_pool_e = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * 4, 1000)

# %% ../notebooks/resnet.ipynb 6
ResNet(
    [
        Layer(3, [64, 64, 256], [1, 3, 1], [1, 2, 1], [0, 1, 0]),
        Layer(4, [128, 128, 512], [1, 3, 1], [1, 2, 1], [0, 1, 0]),
        Layer(6, [256, 256, 1024], [1, 3, 1], [1, 2, 1], [0, 1, 0]),
        Layer(3, [512, 512, 2048], [1, 3, 1], [1, 2, 1], [0, 1, 0]),
    ]
)
