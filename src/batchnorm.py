# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/batchnorm.ipynb.

# %% auto 0
__all__ = ['BatchNorm']

# %% ../notebooks/batchnorm.ipynb 1
import torch
from torch import nn
import torchvision
import torchvision.transforms as transforms

# %% ../notebooks/batchnorm.ipynb 5
class BatchNorm(nn.Module):
    def __init__(self, num_channels=3, eps=1e-5, momentum=0.1):
        super().__init__()
        self.eps = eps
        self.m = momentum # determines how fast EMA smooths
        self.gamma = nn.Parameter(torch.ones(1, num_channels, 1, 1))
        self.beta = nn.Parameter(torch.zeros(1, num_channels, 1, 1))
        self.register_buffer('exponential_mean', torch.zeros(1, num_channels, 1, 1))
        self.register_buffer('exponential_var', torch.ones(1, num_channels, 1, 1))

    def forward(self, X):
        epsilon = self.eps

        if self.training:
            # batch dims: 64 * 3 chan * 32 * 32
            u = torch.mean(X, dim=(0,2,3), keepdim=True)
            var = torch.var(X, dim=(0,2,3), keepdim=True, unbiased=False)

            with torch.no_grad():
                self.exponential_mean.mul_(1 - self.m).add_(self.m * u)
                self.exponential_var.mul_(1 - self.m).add_(self.m * var)
        else:
            u = self.exponential_mean
            var = self.exponential_var

        # mean across the entire block (per channel)
        norm = (X - u) / torch.sqrt(var + epsilon)

        return (self.gamma * norm) + self.beta

